{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install 'gymnasium[box2d]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "from tqdm import *\n",
    "from ema_pytorch import EMA\n",
    "from gymnasium.wrappers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id):\n",
    "    def wrapped_env():\n",
    "        env = gym.make(env_id, render_mode='rgb_array')\n",
    "        return env\n",
    "    return wrapped_env\n",
    "\n",
    "env_id = 'CartPole-v1'\n",
    "env = make_env(env_id)()\n",
    "obs = env.reset()\n",
    "\n",
    "\n",
    "img = env.render()\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape = env.observation_space.shape\n",
    "action_shape = env.action_space.shape\n",
    "n_action = env.action_space.n\n",
    "init_obs, _ = env.reset()\n",
    "\n",
    "print(init_obs.shape)\n",
    "print('obs_shape:', obs_shape)\n",
    "print('action_shape:', action_shape)\n",
    "print('n_action:', n_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return Categorical(logits = logits)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "episodes = 200\n",
    "env = make_env(env_id)()\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = 'cpu'\n",
    "\n",
    "actor = Actor(4, 2).to(device)\n",
    "critic = Critic(obs_shape[0]).to(device)\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(1, episodes + 1), desc=\"A2C training progress: ~~\"):          \n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    frame = 0\n",
    "    while not done:\n",
    "        obs = torch.tensor(obs).to(device)\n",
    "        dist = actor(obs)\n",
    "        value = critic(obs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        next_obs, reward, done, _, info = env.step(action.detach().cpu().numpy())\n",
    "      \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        obs = next_obs\n",
    "        \n",
    "        frame += 1\n",
    "        if frame >= 300:\n",
    "            # Give some constraints to the length of the episode\n",
    "            # otherwise, it will become too large.\n",
    "            break   \n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "        \n",
    "    returns = torch.tensor(returns).to(device)\n",
    "    values = torch.stack(values).to(device)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step() # update critic\n",
    "    \n",
    "    actor_loss = (-torch.stack(log_probs)*advantage.detach()).mean()\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optim.step() # update actor\n",
    "\n",
    "    if episode % 20 == 0:\n",
    "        torch.save(actor.state_dict(), f\"./weight_epoch.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "PATH = \"./weight_epoch.pt\"\n",
    "actor = Actor(4, 2).to(device)\n",
    "actor.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record_modified(env_names, model, episodes=1, filename='gameplay.gif'):\n",
    "    # Create multiple environments\n",
    "    envs = [make_env(name)() for name in env_names]\n",
    "    frames = []\n",
    "    for episode in range(episodes):\n",
    "        obses = [env.reset()[0] for env in envs]\n",
    "        done = [False] * len(envs)\n",
    "        num_frame = 0\n",
    "        while not all(done):\n",
    "            merged_frame = None \n",
    "            \n",
    "            for i, env in enumerate(envs):\n",
    "                if not done[i]:\n",
    "                    obs = torch.FloatTensor(obses[i]).unsqueeze(0).to(device) \n",
    "                    dist = model(obs)  # Get action from your model\n",
    "                    action = dist.sample()[0]\n",
    "                    obses[i], _, done[i], _, _ = env.step(action.cpu().numpy())\n",
    "                frame = env.render()\n",
    "                if merged_frame is None:\n",
    "                    merged_frame = frame\n",
    "                else:\n",
    "                    merged_frame = np.concatenate((merged_frame, frame), axis=1)\n",
    "                    \n",
    "            # Convert array to PIL Image and then append to frames list\n",
    "            frames.append(Image.fromarray(merged_frame))\n",
    "            \n",
    "            for env in envs:\n",
    "                if all(done):\n",
    "                    env.close()\n",
    "    # Save frames as GIF\n",
    "    imageio.mimsave(filename, frames, fps=30)\n",
    "\n",
    "# Example usage\n",
    "actor.eval()\n",
    "env_names = ['CartPole-v1', 'CartPole-v1', 'CartPole-v1'] \n",
    "play_and_record_modified(env_names, actor, episodes=1, filename='./CartPole-v1.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
