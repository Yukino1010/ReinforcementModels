{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install swig \n",
    "#!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ema_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import *\n",
    "from ema_pytorch import EMA\n",
    "from torch.distributions.categorical import Categorical\n",
    "from gymnasium.wrappers import *\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id):\n",
    "    def wrapped_env():\n",
    "        env = gym.make(env_id, render_mode='rgb_array', continuous=False)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = ResizeObservation(env, 84)\n",
    "        env = GrayScaleObservation(env)\n",
    "        env = FrameStack(env, 4)\n",
    "        return env\n",
    "    return wrapped_env\n",
    "\n",
    "env_id = \"CarRacing-v2\"\n",
    "num_envs = 4\n",
    "envs = DummyVecEnv([make_env(env_id) for _ in range(num_envs)])\n",
    "\n",
    "obs = envs.reset()\n",
    "img = envs.render()\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('./car_init.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape = envs.observation_space.shape\n",
    "action_shape = envs.action_space.shape\n",
    "n_action = envs.action_space.n\n",
    "init_obs = envs.reset()\n",
    "\n",
    "print(init_obs.shape)\n",
    "print('obs_shape:', obs_shape)\n",
    "print('action_shape:', action_shape)\n",
    "print('n_action:', n_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Common layers\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, kernel_size=7, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 5 * 5, 512)\n",
    "        self.actor_fc = nn.Linear(512, num_actions)\n",
    "        self.critic_fc = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        x = x.view(-1, 128 * 5 * 5)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.actor_fc(x) # Actor\n",
    "        state_values = self.critic_fc(x) # Critic\n",
    "        return Categorical(logits=logits), state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "gae_lambda = 0.95\n",
    "clip_coef = 0.2\n",
    "ent_coef = 0.01\n",
    "vf_coef = 1 \n",
    "\n",
    "num_envs = 4\n",
    "num_steps = 512\n",
    "minibatch_size = 32\n",
    "total_steps = 500000\n",
    "batch_size = num_envs * num_steps\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "agent = ActorCritic(4, n_action).to(device)\n",
    "ema = EMA(agent, beta = 0.995, update_every = 1)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=1e-4, eps=1e-5)\n",
    "v_loss_fn = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "obs = torch.zeros((num_steps, num_envs) + obs_shape).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs) + action_shape).to(device)\n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "values = torch.zeros((num_steps, num_envs)).to(device)\n",
    "\n",
    "global_step = 0\n",
    "next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "num_updates = total_steps // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = './model_weight/'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "reward_record = []\n",
    "\n",
    "for i in tqdm(range(1, num_updates + 1), desc=\"PPO training progress: ~~\"):\n",
    "    for step in range(num_steps):\n",
    "        global_step += 1 * num_envs\n",
    "        obs[step] = next_obs\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            dist, value = agent(next_obs)\n",
    "            values[step] = value.view(-1)\n",
    "            action = dist.sample()\n",
    "            logprob = dist.log_prob(action)\n",
    "\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "        next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        dones[step] = torch.Tensor(done).to(device)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs = torch.Tensor(next_obs).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, next_value = agent(next_obs)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                is_done = 1.0 - dones[t]\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                is_done = 1.0 - dones[t]\n",
    "                nextvalues = values[t+1]\n",
    "            delta = rewards[t] + gamma * nextvalues * is_done - values[t]\n",
    "            advantages[t] = lastgaelam = delta + gamma * gae_lambda * is_done * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    b_obs = obs.view((-1,) + obs_shape)\n",
    "    b_actions = actions.view((-1,) + action_shape)\n",
    "    b_logprobs = logprobs.view(-1)\n",
    "    b_advantages = advantages.view(-1)\n",
    "    b_returns = returns.view(-1)\n",
    "\n",
    "    b_inds = np.arange(batch_size)\n",
    "    for epoch in range(4):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            dist, new_value = agent(b_obs[mb_inds])\n",
    "            newlogprob = dist.log_prob(b_actions.long()[mb_inds])\n",
    "            entropy = dist.entropy()\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            # Policy loss\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            v_loss = v_loss_fn(newvalue, b_returns[mb_inds])\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            ema.update()\n",
    "    \n",
    "    reward_record.append(b_returns.mean().item())\n",
    "    torch.save(ema.state_dict(), f\"{ckpt_dir}/ema_epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(reward_record)\n",
    "plt.plot(reward_record)\n",
    "plt.title('cumalative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./result/', exist_ok=True)\n",
    "env_id = \"CarRacing-v2\"\n",
    "num_envs = 4\n",
    "envs = DummyVecEnv([make_env(env_id) for _ in range(num_envs)])\n",
    "\n",
    "def record_video(env, policy, out_directory, fps=20):\n",
    "    images = []  \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "    \n",
    "    i = 0\n",
    "    while not done:\n",
    "        i += 1\n",
    "        state = torch.Tensor(state).to(device)\n",
    "        dist, _ = policy(state)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        state, reward, done, info = env.step(action.cpu().numpy()) \n",
    "        img = env.render()\n",
    "        done = any(done)\n",
    "        #print(img.shape)\n",
    "        images.append(img)\n",
    "        if i > 2000:\n",
    "            break\n",
    "    imageio.mimsave('./result/test.gif', [np.array(img) for _, img in enumerate(images)], fps=fps)\n",
    "ema.eval()\n",
    "record_video(envs, ema, out_directory = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
