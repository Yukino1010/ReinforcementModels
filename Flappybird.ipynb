{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ema_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import flappy_bird_gymnasium\n",
    "from ema_pytorch import EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id):\n",
    "    def wrapped_env():\n",
    "        env = gym.make(env_id, render_mode='rgb_array')\n",
    "        # could add some environment wrapper\n",
    "        return env\n",
    "    return wrapped_env\n",
    "\n",
    "env_id = 'FlappyBird-v0'\n",
    "env = make_env(env_id)()\n",
    "obs, _ = env.reset()\n",
    "image = env.render()\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BirdPolicy, self).__init__()\n",
    "        self.fc1 = nn.Linear(180, 128)  \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 128)  \n",
    "        self.relu2 = nn.ReLU()   \n",
    "        self.fc3 = nn.Linear(128, 2)  \n",
    "        self.softmax = nn.Softmax(dim=-1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_traj(env, policy, device):\n",
    "    obs, _ = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        obs = torch.FloatTensor(obs).unsqueeze(0).to(device) # Add batch dimension\n",
    "        probs = policy(obs)\n",
    "        action = torch.distributions.Categorical(probs).sample()\n",
    "        log_prob = torch.log(probs.squeeze(0)[action])\n",
    "        obs, reward, done, _, info = env.step(action.item())\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "    return log_probs, rewards\n",
    "\n",
    "def REINFORCE(env, episodes, policy, ema, device, optimizer):\n",
    "    for episode in range(episodes):\n",
    "        Return = []\n",
    "        Gt = 0\n",
    "        # collect trajectory\n",
    "        log_probs, rewards = collect_traj(env, policy, device)\n",
    "\n",
    "        for reward in rewards[::-1]:\n",
    "            # bellman equation\n",
    "            # computed from back to front\n",
    "            Gt = reward + 0.99 * Gt\n",
    "            Return.insert(0, Gt)\n",
    "        \n",
    "        Return = torch.tensor(Return).to(device)\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, Return):\n",
    "            # compute policy gradient\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.update()\n",
    "            \n",
    "        if episode % 1000 == 0:\n",
    "            log_probs, rewards = collect_traj(env, ema, device)\n",
    "            print(f'Episode {episode+1}, Total Reward: {sum(rewards)}')\n",
    "            torch.save(ema.state_dict(), f\"./weight_epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "policy = BirdPolicy().to(device) \n",
    "ema = EMA(policy, beta = 0.99, update_every = 1)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "\n",
    "# train for 20000 iterations\n",
    "REINFORCE(env, 20000, policy, device, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = f\"{ckpt_dir}/weight_epoch.pt\"\n",
    "PATH = \"./weight_epoch.pt\"\n",
    "policy = BirdPolicy().to(device) \n",
    "ema = EMA(policy, beta = 0.99, update_every = 1)\n",
    "ema.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record_modified(env_names, model, episodes=1, filename='gameplay.gif'):\n",
    "    # Create multiple environments\n",
    "    envs = [make_env(name)() for name in env_names]\n",
    "    frames = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obses = [env.reset()[0] for env in envs]\n",
    "        done = [False] * len(envs)\n",
    "        while not all(done):\n",
    "            merged_frame = None \n",
    "            \n",
    "            for i, env in enumerate(envs):\n",
    "                if not done[i]:\n",
    "                    obs = torch.FloatTensor(obses[i]).unsqueeze(0).to('cuda') \n",
    "                    probs = model(obs)  # Get action from your model\n",
    "                    action = torch.distributions.Categorical(probs).sample()\n",
    "                    obses[i], _, done[i], _, _ = env.step(action.cpu().numpy())\n",
    "                frame = env.render()\n",
    "                if merged_frame is None:\n",
    "                    merged_frame = frame\n",
    "                else:\n",
    "                    merged_frame = np.concatenate((merged_frame, frame), axis=1)\n",
    "                    \n",
    "            # Convert array to PIL Image and then append to frames list\n",
    "            frames.append(Image.fromarray(merged_frame))\n",
    "            for env in envs:\n",
    "                if all(done):\n",
    "                    env.close()\n",
    "\n",
    "    # Save frames as GIF\n",
    "    imageio.mimsave(filename, frames, fps=30)\n",
    "# Example usage\n",
    "env_names = ['FlappyBird-v0', 'FlappyBird-v0', 'FlappyBird-v0']  # The environments you want to run\n",
    "play_and_record_modified(env_names, policy, episodes=1, filename='FlappyBird_Triple.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
